{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD MODULES\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np   \n",
    "import imp\n",
    "import vcf \n",
    "import math\n",
    "import pickle \n",
    "import xgboost as xgb \n",
    "import f  # (local script bank)\n",
    "import random\n",
    "try:\n",
    "    import shap\n",
    "    featimpflag=1\n",
    "except:\n",
    "    print(\"shap python package not found...will not compute feature importances.\")\n",
    "    featimpflag=0\n",
    "#loc_cmd='python /igm/home/jbg001/git/germfilt/germline_variant_filtering/kf/autofilt_train.py --traindir /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_3_combo_1_prepy_0 --mod_paramdict /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_3_combo_1_prepy_0/mod_params.json --vcf_query_preprocessed /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_3_giab_3/query_preprocessed_ph.vcf.gz --vcf_happy /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_3_giab_3/happyout_ph.vcf.gz'\n",
    "#loc_cmd='python /igm/home/jbg001/git/germfilt/germline_variant_filtering/kf/autofilt_train.py --traindir /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_1_combo_2_prepy_0 --mod_paramdict /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_1_combo_2_prepy_0/mod_params.json --vcf_query_preprocessed /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_1_giab_3/query_preprocessed_ph.vcf.gz --vcf_happy /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_1_giab_3/happyout_ph.vcf.gz'\n",
    "#loc_cmd='python /igm/home/jbg001/git/germfilt/germline_variant_filtering/kf/autofilt_train.py --traindir /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_2_combo_1_prepy_0 --mod_paramdict /igm/projects/FOR_JEFF/germfilt/train/fulltrain_030920_afternoon_batch_2_combo_1_prepy_0/mod_params.json --vcf_query_preprocessed /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_2_giab_1/query_preprocessed_ph.vcf.gz --vcf_happy /igm/projects/FOR_JEFF/germfilt/tm/03062020_noon_noprepy_batch_2_giab_1/happyout_ph.vcf.gz'\n",
    "filestem='run_train_5_1_0'\n",
    "testargs=f.grab_bashscript_args(filestem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ INPUT PARAMS, DEFINE OUTFILES\n",
    "if 1==1:\n",
    "    myargs=sys.argv\n",
    "    myargs=testargs\n",
    "    querylocs=[x+1 for x in range(len(myargs)) if myargs[x]=='--vcf_query_preprocessed']\n",
    "    haplocs=[x+1 for x in range(len(myargs)) if myargs[x]=='--vcf_happy']\n",
    "    if len(querylocs) != len(haplocs):\n",
    "        print(\"Error: # arguments preceded by --vcf_query_preprocessed must equal # arguments preceded by --vcf-happy\")\n",
    "    input_vcf_dict={i:{'query':myargs[querylocs[i]],   'happy':myargs[haplocs[i]] } for i in range(len(querylocs))}\n",
    "    if '--traindir' in myargs:\n",
    "        traindir=myargs[myargs.index('--traindir')+1]\n",
    "    else:\n",
    "        print(\"Error - missing training directory parameter:  should follow --traindir in command\")\n",
    "    if '--mod_paramdict' in myargs:\n",
    "        jsonfile=myargs[myargs.index('--mod_paramdict')+1]\n",
    "    else:\n",
    "        print(\"Warning - No json model param dict provided...using default parameters.\")\n",
    "else:\n",
    "    input_vcf_dict={0:{'query':'/igm/projects/FOR_JEFF/germfilt/tm/run_02272020/query_preprocessed_jeffsrun.vcf.gz',\n",
    "                       'happy':'/igm/projects/FOR_JEFF/germfilt/tm/run_02272020/happyout_jeffsrun.vcf.gz'}}\n",
    "    traindir='/igm/projects/FOR_JEFF/germfilt/tm/run_02272020/training_022720_beforelunch'\n",
    "    #jsonfile='/igm/projects/FOR_JEFF/germfilt/tm/batch_1_giab_1_02262020_v2/training/mod_params.json'\n",
    "perffile=traindir+'/allmod_summary.csv'\n",
    "featimpfile=traindir+'/feature_importances.csv'\n",
    "joincols=['CHROM','POS','REF']\n",
    "moddir=traindir+'/mod'\n",
    "f.quietly_create_directory(traindir)\n",
    "f.quietly_create_directory(moddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET MODEL PARAMETERS\n",
    "imp.reload(f)\n",
    "jdict={1:'1_5',6:'6_15',16:'16_PLUS'}\n",
    "vartypes=['SNP']+[x+jdict[j] for x in ['I','D'] for j in [1,6,16]]+['MNP']\n",
    "req_cols=['modnum','vartype','zyg','moddir_loc']\n",
    "default_paramvals=f.grab_default_procedural_param_dict()\n",
    "#default_paramvals['n_estimators']=500\n",
    "## GRAB PARAMDICT   \n",
    "if jsonfile is not None:\n",
    "    mod_paramdict=f.jread(jsonfile)\n",
    "else:\n",
    "    mod_paramdict=f.grab_mod_paramdict_generic(traindir)\n",
    "# VALIDATE, ADD DEFAULT VALUES\n",
    "for i in mod_paramdict:\n",
    "    for y in req_cols:\n",
    "        if y not in mod_paramdict[i]:\n",
    "            print(\"Error... model parameter-set \"+str(i)+\" is missing required parameter \"+y)\n",
    "    for z in default_paramvals:\n",
    "        if z not in mod_paramdict[i]:\n",
    "            mod_paramdict[i][z]=default_paramvals[z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD VCFS\n",
    "imp.reload(f)\n",
    "vcfdict={}\n",
    "formatted_df_dict={}\n",
    "sampdict={}\n",
    "trainflag=1\n",
    "for i in input_vcf_dict:\n",
    "    queryfile=input_vcf_dict[i]['query']\n",
    "    truthfile=input_vcf_dict[i]['happy']\n",
    "     ## set trainflag=1 to require exactly one sample\n",
    "    ## READ RAW QUERY...ASSIGN ZYG\n",
    "    queryfile=input_vcf_dict[0]['query']\n",
    "    vcfdict['query']=f.grab_query_vcf_new(queryfile,trainflag)\n",
    "    dfraw=vcfdict['query']['df']\n",
    "    sampdict[i]=vcfdict['query']['samples'][0]\n",
    "    samp=sampdict[i]\n",
    "    vcfdict['query']['keepinds']=f.add_zyg(dfraw,'gtfeat_'+samp+'_GT')\n",
    "    dfpre=dfraw.loc[vcfdict['query']['keepinds']]\n",
    "    ## ASSIGN VARTYPE\n",
    "    f.add_vartype(dfpre)\n",
    "    ## HC OUTPUT\n",
    "    vcfdict['happy']=f.grab_happy_vcf_new(truthfile)\n",
    "    hfraw=vcfdict['happy']['df']\n",
    "    hfpre=hfraw.loc[hfraw[hfraw['ALT_list'].apply(len)==1].index]\n",
    "    hfpre['ALT']=hfpre['ALT_list'].apply(lambda x: str(x[0]))\n",
    "    ## JOIN\n",
    "    refcols=['CHROM','POS','REF','ALT']\n",
    "    df=pd.merge(left=dfpre.assign(queryflag=1),\n",
    "              right=hfpre.assign(truthflag=1).drop(columns=[x for x in hfpre.columns if x in dfpre.columns and x not in refcols]),\n",
    "              on=refcols,how='inner')\n",
    "    ## DEFINE TRUTH VALUES\n",
    "    df.loc[df['BD_TRUTH'].isnull(),'BD_TRUTH']='X'\n",
    "    zyg_mismatch_inds=df[((df.BD_QUERY=='FP') & (df.BD_TRUTH=='FN')) & (df.BLT_TRUTH != df.BLT_QUERY)].index\n",
    "    df['y']=0\n",
    "    df.loc[df.BD_QUERY=='TP','y']=1\n",
    "    df.loc[zyg_mismatch_inds,'y']=1\n",
    "    formatted_df_dict[i]=df.copy()\n",
    "\n",
    "## COMBINE VCF DATA\n",
    "df=pd.concat([formatted_df_dict[i].rename(columns={x:x.replace(sampdict[i],sampdict[0]) for x in formatted_df_dict[i].columns})  for i in range(len(formatted_df_dict))]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD/FORMAT FEATS\n",
    "## FORMAT sample-specific df (with zygosity)\n",
    "imp.reload(f)\n",
    "df,sampfeats=f.grab_basefeats(df)\n",
    "cursamp=sampdict[0]\n",
    "featdict={x:x.replace('gtfeat_'+cursamp,'gtfeat') for x in sampfeats if 'gtfeat_'+cursamp in x or 'infofeat_' in x or 'genfeat_' in x}\n",
    "gf=df.rename(columns=featdict)\n",
    "adhocfeats=['gtfeat_total_AD']\n",
    "## ADD TOTAL AD... we require AD annotation (but this seems reasonable)\n",
    "gf['gtfeat_total_AD']=gf[[x for x in gf.columns if 'gtfeat_AD' in x]].apply('sum',axis=1)\n",
    "## ADD SS zygosity\n",
    "gtcol='gtfeat_'+cursamp+'_GT'\n",
    "gf['zyg']='het'\n",
    "gf.loc[gf[gtcol].apply(lambda x: (x.count('1'))>1),'zyg']='hom'\n",
    "globfeats=[featdict[x] for x in featdict]+adhocfeats\n",
    "cutfeats=['gtfeat_PS']\n",
    "globfeats=[x for x in globfeats if x not in cutfeats]\n",
    "catfeats_hardcoded=['genfeat_REF', 'genfeat_ALT']\n",
    "for x in [y for y in globfeats if y not in catfeats_hardcoded]:\n",
    "    gf[x]=gf[x].apply(lambda x: float(x) if x is not None else x)\n",
    "globfeats=[x for x in globfeats if 'VQS' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INDICES\n",
    "indict={}\n",
    "for vartype in vartypes:\n",
    "    indict[vartype]=gf[gf.vartype==vartype].index\n",
    "for zyg in ['hom','het']:\n",
    "    indict[zyg]=gf[gf.zyg==zyg].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_SNP_het_0/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_SNP_hom_1/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I1_5_het_2/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I1_5_hom_3/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I6_15_het_4/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I6_15_hom_5/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I16_PLUS_het_6/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_I16_PLUS_hom_7/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D1_5_het_8/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D1_5_hom_9/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D6_15_het_10/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D6_15_hom_11/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D16_PLUS_het_12/mod_0.pkl\n",
      "Wrote model to file /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/mod/mod_D16_PLUS_hom_13/mod_0.pkl\n",
      "Wrote performance file to /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/allmod_summary.csv\n",
      "Wrote feature-importance file to /igm/projects/FOR_JEFF/germfilt/train/granular_train_032020_afternoon_batch_5_combo_1_prepy_0/feature_importances.csv\n"
     ]
    }
   ],
   "source": [
    "## MAIN MODEL LOOP\n",
    "imp.reload(f)\n",
    "perfdict={}\n",
    "perfflag=1\n",
    "zygperfdict={}\n",
    "lc=0\n",
    "shapdict={}\n",
    "moddir=traindir+'/mod'\n",
    "f.quietly_create_directory(moddir)\n",
    "## RESTRICT BY VARTYPE\n",
    "#for vartype in ['SNP','INDEL']:\n",
    "#for vartype in [x for x in vartypes if x != 'SNP']:\n",
    "for vartype in vartypes:\n",
    "    vardat=gf.loc[indict[vartype]]\n",
    "    ## FORMAT BY VARTYPE\n",
    "    if vartype=='SNP':\n",
    "        locfeats=[x for x in globfeats if x not in ['genfeat_REF_len','genfeat_ALT_len']]\n",
    "        catfeats=catfeats_hardcoded\n",
    "    else:\n",
    "        locfeats=[x for x in globfeats if x not in catfeats_hardcoded]\n",
    "        catfeats=[]\n",
    "    ## ONEHOT\n",
    "    ohfeats=f.onehot_encode_cold(vardat,catfeats)\n",
    "    modfeats=[x for x in locfeats if x not in catfeats]+ohfeats\n",
    "    ## RESTRICT BY ZYG\n",
    "    for zyg in ['het','hom']:\n",
    "#    for zyg in ['het']:\n",
    "        locdat_base=vardat.loc[vardat.index.intersection(indict[zyg])]\n",
    "        locdat_base=locdat_base[(locdat_base.y >=0)]\n",
    "        modinds_mid=[x for x in mod_paramdict if ((mod_paramdict[x]['zyg']==zyg) and (mod_paramdict[x]['vartype']==vartype))]\n",
    "        if len(modinds_mid)==0:\n",
    "            print(\"No models for \" + vartype + \" \"+zyg+\"...skipping\")\n",
    "            continue\n",
    "        constrained_paramdat=pd.DataFrame({x:mod_paramdict[x] for x in modinds_mid}).transpose()[['nfold','train_prop','training_depth_cutoff','seed']]\n",
    "        distinct_depth_cutoffs=list(set([mod_paramdict[x]['training_depth_cutoff'] for x in mod_paramdict]))\n",
    "        distinct_depth_cutoff_inddict={x:locdat_base[locdat_base.gtfeat_total_AD >= x].index for x in distinct_depth_cutoffs}\n",
    "        for depth in distinct_depth_cutoffs:\n",
    "            modinds_loc=[x for x in modinds_mid if mod_paramdict[x]['training_depth_cutoff']==depth]\n",
    "            locdat=locdat_base.loc[distinct_depth_cutoff_inddict[depth]]\n",
    "            if len(locdat)==0:\n",
    "                continue\n",
    "            distinct_trainprops=list(set([mod_paramdict[x]['train_prop'] for x in modinds_loc]))\n",
    "            for trainprop in distinct_trainprops:\n",
    "                modinds_prop=[x for x in modinds_loc if mod_paramdict[x]['train_prop']==trainprop]\n",
    "                fold_dict_glob=f.grab_traininds(locdat,trainprop)\n",
    "                distinct_max_trainsizes=list(set([mod_paramdict[x]['max_trainsize'] for x in modinds_prop]))\n",
    "                for max_trainsize in distinct_max_trainsizes:\n",
    "                    modinds_maxtrainsize=[x for x in modinds_prop if mod_paramdict[x]['max_trainsize']==max_trainsize]\n",
    "                    nfold_max=max([mod_paramdict[x]['nfold'] for x in modinds_maxtrainsize])\n",
    "                    if (nfold_max > len(fold_dict_glob)):\n",
    "                        print(\"Requested number of folds \"+str(nfold_max) + \" exceeds that permitted by training proportion \"+str(trainprop)+\". Only running \"+str(len(fold_dict_glob)) +\" folds.\")\n",
    "                        nfold_max=len(fold_dict_glob)\n",
    "                    fold_dict=fold_dict_glob.copy()\n",
    "                    #print(\"Handling models with depth-cutoff \"+str(depth)+\", trainprop \"+str(trainprop) + \", max_trainsize \"+str(max_trainsize))\n",
    "                    for fold in range(nfold_max):\n",
    "                        traindat=locdat.loc[fold_dict[fold]['train']]\n",
    "                        if len(traindat)>max_trainsize:\n",
    "                            traindat=traindat.sample(max_trainsize)\n",
    "                        valdat=locdat.loc[fold_dict[fold]['val']]\n",
    "                        for j in modinds_maxtrainsize:\n",
    "                            paramdict=mod_paramdict[j]\n",
    "                            moddir_loc=paramdict['moddir_loc']\n",
    "                            f.quietly_create_directory(moddir_loc)\n",
    "                            specs=[x for x in paramdict.keys() if x in xgb.XGBClassifier()._get_param_names()]\n",
    "                            xgbdict=xgb.XGBClassifier().get_params()\n",
    "                            for y in specs:\n",
    "                                xgbdict[y]=paramdict[y]\n",
    "                            mymod=xgb.XGBClassifier(**xgbdict)\n",
    "                            #print(\"Training\")\n",
    "                            #g.grabtime()\n",
    "                            mymod.fit(traindat[modfeats],traindat['y'])\n",
    "                            modfile=moddir_loc+'/mod_'+str(fold)+'.pkl'\n",
    "                            print(\"Wrote model to file \"+modfile)\n",
    "                            sys.stdout.flush()\n",
    "                            #g.grabtime()\n",
    "                            pickle.dump(mymod, open(modfile, \"wb\"),protocol=2)\n",
    "                            valdat['yprob_'+str(j)]=mymod.predict_proba(valdat[modfeats])[:,1]\n",
    "                        if perfflag==1:\n",
    "                            perfdat=f.grab_modperf_manymods(valdat,'y',paramdict['PASS_cutoffs'],modinds_loc,vartype,zyg).assign(\n",
    "                                vartype=vartype,zyg=zyg,modfold=fold,modfile=modfile)\n",
    "                            perfdict[lc]=perfdat;lc+=1\n",
    "                            ## FEATURE IMPORTANCES\n",
    "                        if featimpflag==1:\n",
    "                            explainer=shap.TreeExplainer(mymod)\n",
    "                            shapdat_loc=pd.DataFrame(explainer.shap_values(locdat[modfeats]),columns=['shap_'+x for x in modfeats])\n",
    "                            shapdat=shapdat_loc.apply('mean',0).reset_index().rename(columns={'index':'shapfeat',0:'shapval'})\n",
    "                            shapdat['featnice']=shapdat['shapfeat'].apply(lambda x: x.replace('shap_genfeat_','').replace('shap_gtfeat_','').replace('shap_infofeat_',''))\n",
    "                            shapdat['shapval_abs']=shapdat['shapval'].apply(abs)\n",
    "                            shapdat=shapdat.sort_values('shapval_abs',ascending=False).assign(\n",
    "                                ).assign(vartype=vartype,zyg=zyg,modnum=paramdict['modnum'],modfold=fold);\n",
    "                            shapdict[lc]=shapdat[['featnice','shapval','vartype','zyg']].rename(columns={'featnice':'feat'}).assign(modnum=paramdict['modnum'])\n",
    "## SAVE PERFORMANCE REPORT\n",
    "if perfflag==1:\n",
    "    perfdat=pd.concat(perfdict,ignore_index=True)\n",
    "    perfdat=perfdat.sort_values(['vartype','zyg','modid','thresh','modfold'],ascending=[False,True,True,True,True])\n",
    "    perfdat.to_csv(perffile,index=False)\n",
    "    print(\"Wrote performance file to \"+perffile)\n",
    "## CONSOLIDATE FEATURE IMPORT\n",
    "if featimpflag==1:\n",
    "    featimpdat=f.grab_featimp(shapdict)\n",
    "    featimpdat.to_csv(featimpfile,index=False)\n",
    "    print(\"Wrote feature-importance file to \"+featimpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rnaenv]",
   "language": "python",
   "name": "conda-env-rnaenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
